ğŸ“˜ Proyecto Final â€“ Arquitectura MLOps Completa

Este repositorio documenta el desarrollo de un sistema MLOps moderno basado en:

Airflow (ingesta â†’ limpieza â†’ split â†’ entrenamiento)

MLflow (tracking de experimentos y modelos)

MinIO + PostgreSQL (artefactos y metadatos)

FastAPI (servicio de inferencia)

Streamlit (UI de inferencia + explicabilidad)

Docker Compose (entorno local de orquestaciÃ³n del pipeline)

Kubernetes (K3s) desplegado en MV universitaria

GitOps con ArgoCD

PatrÃ³n App-of-Apps (Bootstrap GitOps)

CI/CD con GitHub Actions

El diseÃ±o sigue las especificaciones del proyecto final entregado por el profesor âœ”.

ğŸ—ï¸ 1. Infraestructura Kubernetes + GitOps (completado)
âœ” Cluster Kubernetes (K3s)

Instalado en una MÃ¡quina Virtual universitaria.

kubeconfig configurado y accesible.

Estado validado: nodo en Ready, roles control-plane/master.

âœ” ArgoCD instalado y accesible

Instalado vÃ­a manifiestos oficiales.

Servicio expuesto mediante:

kubectl port-forward svc/argocd-server -n argocd 8000:443
ssh -L 8000:127.0.0.1:8000 estudiante@<IP_MV>


UI accesible desde: https://localhost:8000

âœ” GitOps conectado a GitHub

Repositorio:
ğŸ‘‰ https://github.com/juanruizc1796/mlops-final

ArgoCD conectado vÃ­a HTTPS (repositorio pÃºblico â†’ sin credenciales).

âœ” PatrÃ³n App-of-Apps (Bootstrap GitOps)

Se implementÃ³ arquitectura GitOps profesional:

mlops-final/
â”‚â”€â”€ bootstrap/
â”‚   â”œâ”€â”€ application.yaml    # RaÃ­z GitOps (mlops-bootstrap)
â”‚   â””â”€â”€ apps.yaml           # Define fastapi, streamlit, monitoring, etc.
â”‚
â”‚â”€â”€ apps/
â”‚   â”œâ”€â”€ fastapi/
â”‚   â”œâ”€â”€ streamlit/
â”‚   â”œâ”€â”€ prometheus/
â”‚   â”œâ”€â”€ grafana/
â”‚   â”œâ”€â”€ ingress/
â”‚   â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ database/
â”‚   â””â”€â”€ monitoring/

âœ” Namespaces gestionados por GitOps

ArgoCD creÃ³ automÃ¡ticamente:

fastapi

streamlit

monitoring

database

storage

ingress

Aparecen como la aplicaciÃ³n hija namespaces bajo mlops-bootstrap.

ğŸ³ 2. Infraestructura Docker Compose (Airflow, MLflow, MinIO, PostgreSQL)

El stack local incluye:

Airflow â†’ Orquesta todo el ciclo de datos.

MLflow Tracking Server

Metadatos â†’ PostgreSQL

Artefactos â†’ MinIO bucket

MinIO â†’ Repositorio de artefactos (modelos .pkl, mÃ©tricas, logs)

SQL Server / PostgreSQL â†’ Almacenamiento persistente

Este entorno se usa para el pipeline ETL + entrenamiento, totalmente alineado con la arquitectura del proyecto (ver pÃ¡ginas 3â€“5 del PDF) âœ”.

ğŸ”„ 3. Pipelines en Airflow (DAGs completados)
ğŸ“Œ DAG: house_price_pipeline

Este DAG ejecuta:

fetch_chunk â†’ obtiene un nuevo batch desde la API externa real del profesor:
http://10.43.100.103:8000/data?group_number=1&day=Tuesday
(segÃºn manual del proyecto, pÃ¡g. 2â€“3) âœ”

clean_and_split

Limpieza robusta (validaciÃ³n de columnas, imputaciÃ³n, conversiÃ³n numÃ©rica)

DivisiÃ³n:

70% train

15% validation

15% test

Todo indexado por chunk N para permitir reproducibilidad.

train_model

Entrenamiento acumulado con XGBoostRegressor.

ConcatenaciÃ³n de todos los chunks procesados.

Registro en MLflow:

MÃ©tricas: RMSE, MAE, R2, MAPE

ParÃ¡metros del entrenamiento

Artefactos del modelo guardados en:

MinIO (bucket de artefactos)

Metadatos en PostgreSQL

ğŸ§  MLflow Tracking

Los modelos entrenados quedan registrados por chunk.

El mejor modelo podrÃ¡ marcarse como Production, tal como exigen las reglas del proyecto (pÃ¡g. 5) âœ”

â— Nota sobre MLflow Model Registry

Durante el desarrollo del proyecto se encontrÃ³ un problema al intentar promover modelos al stage Production dentro del MLflow Model Registry.
Este problema afectÃ³ exclusivamente la funcionalidad de promociÃ³n, pero no el tracking de experimentos, mÃ©tricas ni artefactos.

Como soluciÃ³n tÃ©cnica viable y coherente con prÃ¡cticas de MLOps:

â¡ï¸ El modelo final se sirve directamente desde los artefactos locales generados por el pipeline en Airflow, especÃ­ficamente desde:

La API FastAPI fue actualizada para cargar este modelo de forma declarativa desde el sistema de archivos, manteniendo la modularidad y asegurando reproducibilidad del proceso.

## â— Problema conocido: Kubernetes no logra descargar la imagen de FastAPI desde DockerHub

Durante la fase de despliegue en Kubernetes mediante ArgoCD se identificÃ³ un problema relacionado
con la descarga de imÃ¡genes desde DockerHub.

La imagen utilizada para la API (`fastapi-mlops`) fue correctamente construida y publicada en DockerHub:

ğŸ‘‰ https://hub.docker.com/repository/docker/juanse1796/fastapi-mlops/tags/latest/sha256-709e3ab776d95dde281aadd22c63b815b1c9bd53d32130eb035c879e390b52f3

De forma local (Mac), la imagen se descarga sin problemas mediante:

```bash
docker pull juanse1796/fastapi-mlops:latest